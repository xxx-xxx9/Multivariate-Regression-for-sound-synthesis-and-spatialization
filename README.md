Abstract
Traditional synthesis algorithms have for a while been complemented by algorithms which integrate deep learning strategies to sound generation. The present work describes how artificial neural networks, used as tools for Computer-Assisted Composition, are able to control the parameters of a granulator and the spatialization of 30 sources for a d&b Soundscape system. The sound material is mainly vocal and was selected from the archive of a Foundation. That is why the aesthetic approach proposed aims to group grains of different length to develop contrapuntal relationships through the Boids algorithm controlled by one of the two neural networks, while the second one takes care of managing the synthesis parameters of the granulator. Two Multilayer Perceptrons of different depth were implemented in Python to generate predictions, which are sent via OSC protocol to a Max/MSP patch to control synthesis and spatialization parameters. The d&b Soundscape system receives information about the positions of the sources and their gain from the patch. The size of the datasets were kept rather small since a good compromise was found between the prediction error and the ability of the models to generalise when the input data are not part of the train set. Methods such as neurons dropout and regularization in some of the layers were adopted to compensate for that decision and avoid overfitting.
