{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60827eed-64b3-4884-9faf-c9f8bc7f3336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello Synth_model_2x6_27.h5 caricato correttamente.\n",
      "🔵 Server OSC in ascolto su 127.0.0.1:7002...\n",
      "data_buffer  [[0.07086614519357681, 0.0]]\n",
      "✅ Ricevuto da Max/MSP: x = 0.07086614519357681, y = 0.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "x_val -8.915335\n",
      "prefix_x /x0\n",
      "x_val 2.2167804\n",
      "prefix_x /x1\n",
      "x_val 7.598527\n",
      "prefix_x /x2\n",
      "x_val 3.5905406\n",
      "prefix_x /x3\n",
      "x_val 2913.2573\n",
      "prefix_x /x4\n",
      "x_val -5.4399447\n",
      "prefix_x /x5\n"
     ]
    }
   ],
   "source": [
    "# produciamo una predizione sulla base dei dati ricevuti da Max/MSP e ne inviamo il risultato a Max/MSP\n",
    "# USARE qs modello: Boids_225_10.h5\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import socket\n",
    "\n",
    "from pythonosc.udp_client import SimpleUDPClient\n",
    "from pythonosc import osc_message_builder\n",
    "from pythonosc.dispatcher import Dispatcher\n",
    "from pythonosc.osc_server import BlockingOSCUDPServer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "port1 = 7002\n",
    "port2 = 6002\n",
    "ip = \"127.0.0.1\"\n",
    "dim_dati = 9\n",
    "predictions = []\n",
    "values = []\n",
    "data_buffer = []\n",
    "x_vals = []\n",
    "y_vals = []\n",
    "\n",
    "\n",
    "def osc_handler(address, *args):\n",
    "    \"\"\"Funzione che gestisce i dati ricevuti.\"\"\"\n",
    "    # importiamo lo scalaggio introdotto dalla normalizzazione pre-training per \n",
    "    # calcolare la de-normalizzazione post-predizione\n",
    "    y_scaler = joblib.load(\"y_scaler.pkl\")\n",
    "    \n",
    "    global values, data_buffer, x_vals, y_vals\n",
    "    \n",
    "    if len(args) >= 2:\n",
    "        x, y = float(args[0]), float(args[1])\n",
    "        values.append(x)\n",
    "        values.append(y)\n",
    "        data_buffer.append(values)\n",
    "        print(\"data_buffer \", data_buffer)\n",
    "        values=[]\n",
    "        print(f\"✅ Ricevuto da Max/MSP: x = {x}, y = {y}\")\n",
    "        ctrl_data = np.array(data_buffer)  # forma (50, 6) # Se il tuo modello si aspetta un batch di shape (N, 6) va bene così,\n",
    "        predictions = model.predict(ctrl_data)\n",
    "        # NON li riscalo. Tengo le predizioni normalizzate, ossia tra 0 e 1.\n",
    "        #print(\"predictions\", predictions)\n",
    "        #y_pred_scaled = np.array(predictions)\n",
    "        #predictions_original = y_scaler.inverse_transform(y_pred_scaled )\n",
    "        #print(\"predictions_original: \", predictions_original)\n",
    "\n",
    "        client = SimpleUDPClient(ip, port2)  # Crea un client\n",
    "\n",
    "        for ind in range(predictions.shape[1]): # 9\n",
    "            #x_val = predictions_original[0,ind]\n",
    "            x_val = predictions[0,ind]\n",
    "            #print(\"predictions[ind]\",predictions[ind])\n",
    "            print(\"x_val\",x_val)\n",
    "            prefix_x = f\"/x{int(ind)}\"\n",
    "            print(\"prefix_x\", prefix_x)\n",
    "            client.send_message(prefix_x, float(x_val))\n",
    "\n",
    "            x_vals.append(x_val)\n",
    "     \n",
    "    # Svuoto il buffer per il prossimo batch\n",
    "    data_buffer.clear()\n",
    "\n",
    "# Carica il modello Keras salvato in precedenza\n",
    "modello = \"Synth_model_2x6_9.h5\" # funziona BENE!\n",
    "model = load_model(modello)\n",
    "print(\"Modello\", modello, \"caricato correttamente.\")\n",
    "\n",
    "\n",
    "# Avvia l'ascolto dei dati\n",
    "dispatcher = Dispatcher()\n",
    "dispatcher.map(\"/*\", osc_handler)  # Assegna la funzione a tutti i messaggi OSC\n",
    "\n",
    "# Configura il server OSC\n",
    "UDP_IP = \"127.0.0.1\"  # Indirizzo locale\n",
    "UDP_PORT = port1       # Porta su cui Max/MSP sta inviando\n",
    "\n",
    "server = BlockingOSCUDPServer((UDP_IP, UDP_PORT), dispatcher)\n",
    "print(f\"🔵 Server OSC in ascolto su {UDP_IP}:{UDP_PORT}...\")\n",
    "\n",
    "# Avvia il server OSC\n",
    "server.serve_forever()\n",
    "\n",
    "\n",
    "# Avvia l'ascolto dei dati\n",
    "#listen_ctrl_data(port=port1, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a378346a-9bae-4957-9eb8-89ee479030c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
